# Generative AI with LLM (Large Language Models)
- Generative AI and Large Language Models (LLMs) are general-purpose technologies, meaning they are not limited to a single application but can be utilized across a wide range of applications.

__First Week:__
- Examine transformers that power large language models
- Explore how these models are trained
- Understand the compute resources required to develop these powerful LLMs
- Learn about the technique called in-context learning
- Learn how to guide the model to output at inference time with prompt engineering
- Discover how to tune the most important generation parameters of LLMs to refine your model output
- Construct and compare different prompts and inputs for a given generative task, such as dialogue summarization
- Explore different inference parameters and sampling strategies to gain intuition on how to improve generative model responses further

__Second Week:__
- Explore options for adapting pre-trained models to a specific task and dataset via a process called instruction fine-tuning
- Learn how to align the output of language models with human values to increase helpfulness and decrease potential harm and toxicity
- Fine-tune an existing large language model from Hugging Face
- Experiment with both full fine-tuning and parameter-efficient fine-tuning (PEFT) and see how PEFT makes your workflow much more efficient

__Third Week:__
- Get hands-on with reinforcement learning from human feedback (RLHF)
- Build a reward model classifier to label model responses as either toxic or non-toxic

### How transform network actually work?
- Attention is all you need, Self attention, Multi headed self attention
- 
